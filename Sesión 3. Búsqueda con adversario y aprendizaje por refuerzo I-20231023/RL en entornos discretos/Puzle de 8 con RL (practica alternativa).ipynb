{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ea81f93",
   "metadata": {},
   "source": [
    " ## Puzle de 8 con RL\n",
    " \n",
    "Se aplica el algoritmo Q-learning para diseñar un agente que aprenda a resolver el puzzle del 8. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf78d00e",
   "metadata": {},
   "source": [
    "Para poder aplicar Q-learning necesitamos numerar las diferentes acciones y estados para poder crear la q_table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5622145e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a827a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from search import Problem\n",
    "import itertools\n",
    "class Ocho_Puzzle(Problem):\n",
    "    \"\"\"Problema a del 8-puzzle.  Los estados serán tuplas de nueve elementos,\n",
    "    permutaciones de los números del 0 al 8 (el 0 es el hueco). Representan la\n",
    "    disposición de las fichas en el tablero, leídas por filas de arriba a\n",
    "    abajo, y dentro de cada fila, de izquierda a derecha. Las cuatro\n",
    "    acciones del problema las representaremos mediante las cadenas:\n",
    "    \"Mover hueco arriba\", \"Mover hueco abajo\", \"Mover hueco izquierda\" y\n",
    "    \"Mover hueco derecha\", respectivamente.\"\"\"\"\"\n",
    "\n",
    "    def __init__(self, initial, goal=(1, 2, 3, 4, 5, 6, 7, 8, 0)):\n",
    "        \"\"\" Define goal state and initialize a problem \"\"\"\n",
    "        self.goal = goal\n",
    "        Problem.__init__(self, initial, goal)\n",
    "        \n",
    "    \n",
    "    def actions(self,estado):\n",
    "     \n",
    "        pos_hueco=estado.index(0) # busco la posicion del 0\n",
    "        accs=list()\n",
    "        if pos_hueco not in (0,1,2):\n",
    "            accs.append(\"Mover hueco arriba\")\n",
    "        if pos_hueco not in (2,5,8):\n",
    "            accs.append(\"Mover hueco derecha\")\n",
    "        if pos_hueco not in (0,3,6): \n",
    "            accs.append(\"Mover hueco izquierda\") \n",
    "        if pos_hueco not in (6,7,8):\n",
    "            accs.append(\"Mover hueco abajo\")\n",
    "                \n",
    "        return accs     \n",
    "\n",
    "    def result(self,estado,accion):\n",
    "        pos_hueco = estado.index(0)\n",
    "        l = list(estado)\n",
    "        if accion == \"Mover hueco arriba\":\n",
    "            l[pos_hueco] = l[pos_hueco-3]\n",
    "            l[pos_hueco-3] = 0\n",
    "        if accion == \"Mover hueco abajo\":\n",
    "            l[pos_hueco] = l[pos_hueco + 3]\n",
    "            l[pos_hueco + 3] = 0\n",
    "        if accion == \"Mover hueco derecha\":\n",
    "            l[pos_hueco] = l[pos_hueco+1]\n",
    "            l[pos_hueco+1] = 0\n",
    "        if accion == \"Mover hueco izquierda\":\n",
    "            l[pos_hueco] = l[pos_hueco-1]\n",
    "            l[pos_hueco-1] = 0\n",
    "        return tuple(l)\n",
    "    \n",
    "    def h(self, node):\n",
    "        \"\"\" Return the heuristic value for a given state. \"\"\"\n",
    "        return 1\n",
    "    \n",
    "    def check_solvability(self, state):\n",
    "        \"\"\" Checks if the given state is solvable \"\"\"\n",
    "    # The solvability of a configuration can be checked by calculating the Inversion Permutation. \n",
    "    #If the total Inversion Permutation is even then the initial configuration is solvable else the initial configuration is not solvable which means that only 9!/2 initial states lead to a solution.\n",
    "        inversion = 0\n",
    "        for i in range(len(state)):\n",
    "            for j in range(i+1, len(state)):\n",
    "                if (state[i] > state[j]) and state[i] != 0 and state[j]!= 0:\n",
    "                    inversion += 1\n",
    "        return inversion % 2 == 0\n",
    "    \n",
    "    def lista_estados(self,estado):\n",
    "        i = 0\n",
    "        diccionario = {}\n",
    "        for e in itertools.permutations(estado):\n",
    "            diccionario[e] = i\n",
    "            i = i + 1\n",
    "        return diccionario  \n",
    "    \n",
    "    def listaAcciones(self):\n",
    "        listaAcciones = {}\n",
    "        listaAcciones[0] =\"Mover hueco arriba\" \n",
    "        listaAcciones[1] =\"Mover hueco abajo\"\n",
    "        listaAcciones[2] =\"Mover hueco derecha\"\n",
    "        listaAcciones[3] =\"Mover hueco izquierda\"\n",
    "        return listaAcciones\n",
    "    \n",
    "    def listaAcciones2(self):\n",
    "        listaAcciones2 = {}\n",
    "        listaAcciones2[\"Mover hueco arriba\"] = 0\n",
    "        listaAcciones2[\"Mover hueco abajo\"] =1\n",
    "        listaAcciones2[\"Mover hueco derecha\"] =2\n",
    "        listaAcciones2[\"Mover hueco izquierda\"] =3\n",
    "        return listaAcciones2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ec5c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos e inicializamos la Q-Table\n",
    "totalAcciones = 4\n",
    "totalEstados = math.factorial(9)\n",
    "q_table = np.zeros([totalEstados, totalAcciones])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3245eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial=(1,2,0,4,5,3,7,8,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba158298",
   "metadata": {},
   "outputs": [],
   "source": [
    "ocho = Ocho_Puzzle(initial)\n",
    "print(\"estado objetivo\",ocho.goal)   \n",
    "ocho.__init__(initial) \n",
    "state = ocho.initial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88bbcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recompensa(state):\n",
    "    if state==ocho.goal:\n",
    "        return 100\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c18146",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agente_puzle8(episodios=10000, alpha = 0.1, gamma = 0.6, epsilon = 0.8):\n",
    "    \"\"\"Creamos y entrenamos el agente con los parametros dados\"\"\"\n",
    "    state = ocho.initial\n",
    "    listaAcciones2 = ocho.listaAcciones2()\n",
    "    listaAcciones = ocho.listaAcciones()\n",
    "    listaEstados = ocho.lista_estados(state)\n",
    "    exitos=0\n",
    "    for i in range(1, episodios):\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            mejor = listaAcciones2[random.choice(ocho.actions(state))]\n",
    "            #print(\"La accion aleatoria seleccionada es \", listaAcciones[mejor])\n",
    "        else:\n",
    "            mejor = 0\n",
    "            for accion in listaAcciones.keys():\n",
    "                if q_table[listaEstados[state]][accion] > q_table[listaEstados[state]][mejor]:\n",
    "                    mejor = accion\n",
    "                    #print(\"La mejor accion es \", listaAcciones[mejor])\n",
    "\n",
    "        if (listaAcciones[mejor] in ocho.actions(state)):\n",
    "            sig_estado = ocho.result(state , listaAcciones[mejor])\n",
    "            reward = recompensa(sig_estado)\n",
    "        else:\n",
    "            reward = -50 #accion no aplicable\n",
    "            sig_estado =state\n",
    "        old_value = q_table[listaEstados[state]][mejor]\n",
    "        next_max = np.max(q_table[listaEstados[sig_estado]][mejor])\n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[listaEstados[state], mejor] = new_value\n",
    "        state = sig_estado\n",
    "        if state == ocho.goal:\n",
    "            #Termino el episodio con éxito. Se ha encontrado la solución\n",
    "            state = tuple(random.sample(ocho.goal, len(ocho.goal)))\n",
    "            #state = ocho.initial\n",
    "            exitos=exitos+1\n",
    "            continue\n",
    "        if i % 100 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"Episode: {i}\")\n",
    "\n",
    "    print(\"Training finished.\\n\") \n",
    "    print(\"La Q-table:\\n\") \n",
    "    print(q_table)\n",
    "    # Métrica del % de veces que ha llegado al estado objetivo. \n",
    "    print(\"Número de éxitos: \", exitos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf75046d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFICA LOS PARÁMETROS Y OBSERVA EL APRENDIZAJE\n",
    "alpha = 0.1\n",
    "gamma = 0.5\n",
    "epsilon = 0.8\n",
    "episodios=1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272c3933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# puedes medir el tiempo de aprendizaje \n",
    "#%%time\n",
    "agente_puzle8(episodios,alpha,gamma,epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917e100e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalua_agente_puzle8(initial):\n",
    "    \"\"\"Solo explotación del agente sin modificar la tabla Q\"\"\"\n",
    "    exitos=0\n",
    "    fallos=0\n",
    "    state = initial\n",
    "    episodios =100\n",
    "    #listaAcciones2 = ocho.listaAcciones2()\n",
    "    listaAcciones = ocho.listaAcciones()\n",
    "    listaEstados = ocho.lista_estados(state)\n",
    "    for i in range(1, episodios+1):\n",
    "        done =False\n",
    "        while not done:\n",
    "            mejor_accion = 0        \n",
    "            for accion in listaAcciones.keys():\n",
    "                if q_table[listaEstados[state]][accion] > q_table[listaEstados[state]][mejor_accion]:\n",
    "                    mejor_accion = accion                \n",
    "            print(\"La accion aleatoria seleccionada es \", listaAcciones[mejor_accion])\n",
    "            print(\"Estado:\", state)\n",
    "            if (listaAcciones[mejor_accion] in ocho.actions(state)):\n",
    "                state = ocho.result(state , listaAcciones[mejor_accion])\n",
    "                print(\"Estado siguiente:\", state)\n",
    "            else: \n",
    "                #termino con fallo porque la accion no es aplicable\n",
    "                fallos=fallos+1\n",
    "                done = True            \n",
    "            if state == ocho.goal:\n",
    "                #print(\"Termino el episodio con éxito. Se ha encontrado la solución\")\n",
    "                exitos=exitos+1 \n",
    "                done =True \n",
    "                state=initial\n",
    "            if i % 100 == 0:\n",
    "                clear_output(wait=True)\n",
    "                print(f\"Episodio: {i}\")\n",
    "        \n",
    "    print(\"Evaluación terminada.\\n\") \n",
    "    # Métrica del % de veces que ha llegado al estado objetivo. \n",
    "    print(\"Número de éxitos:\", exitos)\n",
    "    print(\"Número de éxitos:\", (exitos/episodios)*100, \" %\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fc740d",
   "metadata": {},
   "outputs": [],
   "source": [
    "evalua_agente_puzle8(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3477321f",
   "metadata": {},
   "source": [
    "El agente ha aprendido con QLearning a resolver el problema del 8 puzle tardando un cierto tiempo.\n",
    "Una vez entrenado puedes ver qué tal se comporta (usando sólo la parte de explotación de la matriz Q aprendida y midiendo el porcentaje de éxitos). \n",
    "\n",
    "#### Responde a las siguientes cuestiones:\n",
    "\n",
    "- Justifica de forma razonada si esta aproximación con aprendizaje es mejor que la aproximación con búsqueda heurística de la primera práctica.\n",
    "- Observa la matriz de aprendizaje ¿qué ha aprendido el agente? __El agente aprende cuál es la mejor opción en cada estado.__\n",
    "- Si entrenas el agente para un estado inicial ¿cómo se comporta al evaluarlo con otro estado inicial? __El número de éxito será muy baja (0) y si el estado inicial es similar al estado inicial origen, el tiempo de ejecución va muy larga.__\n",
    "- Realiza los cambios necesarios para que el agente aprenda a resolver el problema desde cualquier estado inicial. __Cuando el trainning terminar con éxito, no va a volver al mismo estado inicial, va poner un estado cualquier con el codigo \"state = tuple(random.sample(ocho.goal, len(ocho.goal)))\"__\n",
    "- Te parece adecuada la función de recompensa utilizada. ¿Crees que se podría mejorar el rendimiento del agente cambiando la recompensa? __Si, pongo -1 si el estado actual no es el objetivo de estado, para que pueda encontrar la solución con el menor número de operaciones.__\n",
    "- Comprueba cómo se comporta el aprendizaje con distintas configuraciones de los parámetros. Indica claramente cuál es la mejor configuración que has encontrado. __alpha = 0.1, gamma = 0.5, epsilon = 0.8__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f7494c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "c347c8f9a7ef94e4c9e03b4513be7835ed18f45b99a2a817fb579f408b867b16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
